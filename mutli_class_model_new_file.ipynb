{
  "cells": [
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:49.884037Z",
          "start_time": "2025-08-26T13:54:48.749030Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from pandas import DataFrame\n",
        "from torch import nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.330009Z",
          "start_time": "2025-08-26T13:54:49.886600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98121cbedb5d9d3",
        "outputId": "c084b097-accb-425d-ff2f-31004bc865d3"
      },
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "id": "f98121cbedb5d9d3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from torcheval) (4.14.1)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.385264Z",
          "start_time": "2025-08-26T13:54:50.383620Z"
        },
        "id": "e4c34c6c0e28ed53"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) #Numpy and Pandas FutureWarnings - ignore them for now, we can fix them later"
      ],
      "id": "e4c34c6c0e28ed53",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.399351Z",
          "start_time": "2025-08-26T13:54:50.391464Z"
        },
        "id": "f09f9b8a6f72952b"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('categories.csv')\n",
        "df.head()"
      ],
      "id": "f09f9b8a6f72952b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.417889Z",
          "start_time": "2025-08-26T13:54:50.416003Z"
        },
        "id": "b3936f1f300a78f4"
      },
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['location','time_of_day','skyCondition']) # Drop other columns, we will use 'setting' as the target variable, input will be the image."
      ],
      "id": "b3936f1f300a78f4",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.440947Z",
          "start_time": "2025-08-26T13:54:50.437464Z"
        },
        "id": "8d988beb92680e2d"
      },
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=['setting'], dtype='long')\n",
        "df.columns"
      ],
      "id": "8d988beb92680e2d",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.463706Z",
          "start_time": "2025-08-26T13:54:50.460386Z"
        },
        "id": "585dc337dc595946"
      },
      "cell_type": "code",
      "source": [
        "df.drop('setting_abstract', axis=1, inplace=True)\n",
        "df.head()"
      ],
      "id": "585dc337dc595946",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.503601Z",
          "start_time": "2025-08-26T13:54:50.502240Z"
        },
        "id": "a43fdf82a440a400"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "id": "a43fdf82a440a400",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.535706Z",
          "start_time": "2025-08-26T13:54:50.533510Z"
        },
        "id": "80e3e585a420b8b"
      },
      "cell_type": "code",
      "source": [
        "from torchvision.io import decode_image\n",
        "from torchvision.io import ImageReadMode\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.img_dir = img_dir\n",
        "        self.bounds = []\n",
        "\n",
        "        if not self.img_dir:\n",
        "            raise ValueError(f\"No images found in directory: {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_name = row['img_name']  # Assuming the first column is 'img_name'\n",
        "        #Get the label tensor from the row, excluding the first column (img_name)\n",
        "        label = torch.tensor(row[1:], dtype=torch.long)  # Convert the labels to tensor (excluding img_name, which is the input image name)\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name) + \".jpg\" # Full path to the image file\n",
        "        image = decode_image(img_path, mode=ImageReadMode.RGB) #outputs tensor of shape (C, H, W) where C is the number of channels, H is the height and W is the width\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "id": "80e3e585a420b8b",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.563585Z",
          "start_time": "2025-08-26T13:54:50.561231Z"
        },
        "id": "6ccd55e4ef8b145f"
      },
      "cell_type": "code",
      "source": [
        "# Transformations for the dataset\n",
        "\n",
        "# These are the standard transformations for the dataset, we will use these for training, testing, and validation\n",
        "val_test_standard_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
        "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
        "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "id": "6ccd55e4ef8b145f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.591514Z",
          "start_time": "2025-08-26T13:54:50.589087Z"
        },
        "id": "71d5e6ce15be89b1"
      },
      "cell_type": "code",
      "source": [
        "#Doing data augmentation for training set to get more data and prevent overfitting\n",
        "#These are applied to the training set only, not the validation or test set\n",
        "#They are applied dynamically during training, so sample images will be different each time, effectively increasing the size of the training set\n",
        "\n",
        "\n",
        "#Training transformations with data augmentation\n",
        "training_augmentation_transforms = transforms.Compose([\n",
        "    #Standard transformations for the dataset, we will still use these for training (rest below)\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
        "    #Data augmentation transformations for training\n",
        "    # p= probability of applying the transformation, 0.2 means 20% chance of applying the transformation\n",
        "    transforms.RandomHorizontalFlip(p=0.2), # Randomly flip the image horizontally\n",
        "    transforms.RandomVerticalFlip(p=0.2), # Randomly flip the image vertically\n",
        "\n",
        "    #RandomApply makes any transformation randomly applied with a probability p, so we can apply the same transformation with a probability of 0.2\n",
        "\n",
        "    #30% chance of:\n",
        "    transforms.RandomApply(\n",
        "        [\n",
        "            transforms.RandomCrop(size=430, pad_if_needed=True)\n",
        "        ], # Randomly crop the image to 430 in random locations, pad if needed to maintain the size for input to the model (82 pixels on each side, so 82+430+82=512)\n",
        "        p=0.2),\n",
        "\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=0.2), # Randomly adjust the sharpness of the image\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=0.5,p=0.2), # Randomly adjust the sharpness of the image\n",
        "\n",
        "\n",
        "    transforms.RandomGrayscale(p=0.1), # Randomly convert the image to grayscale with a probability of 0.1\n",
        "    #The rest of the standard transformations\n",
        "\n",
        "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
        "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "])"
      ],
      "id": "71d5e6ce15be89b1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.738423Z",
          "start_time": "2025-08-26T13:54:50.601156Z"
        },
        "id": "d530f2890ee254e3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42) # 70% training, 30% test\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split the test set into validation and test sets\n",
        "#This gives us 70% training, 15% validation, and 15% test sets"
      ],
      "id": "d530f2890ee254e3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.750202Z",
          "start_time": "2025-08-26T13:54:50.748546Z"
        },
        "id": "37259b001bc7523c"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_df, img_dir='raw_images_organised', transform=training_augmentation_transforms)  # Create the training dataset with data augmentation\n",
        "test_dataset = ImageDataset(test_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the test dataset with standard transformations\n",
        "val_dataset = ImageDataset(val_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the validation dataset with standard transformations"
      ],
      "id": "37259b001bc7523c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.754692Z",
          "start_time": "2025-08-26T13:54:50.753170Z"
        },
        "id": "86ad59fab4f69ccc"
      },
      "cell_type": "code",
      "source": [
        "print(train_dataset.transform)\n",
        "print(test_dataset.transform)\n",
        "print(val_dataset.transform)"
      ],
      "id": "86ad59fab4f69ccc",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.761763Z",
          "start_time": "2025-08-26T13:54:50.759917Z"
        },
        "id": "c0c0ba7631217bf1"
      },
      "cell_type": "code",
      "source": [
        "#Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "id": "c0c0ba7631217bf1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.868179Z",
          "start_time": "2025-08-26T13:54:50.766608Z"
        },
        "id": "a72726019a18c8ed"
      },
      "cell_type": "code",
      "source": [
        "train_images, train_labels = next(iter(train_loader))\n",
        "print(train_images.shape, train_labels.shape)"
      ],
      "id": "a72726019a18c8ed",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.153624Z",
          "start_time": "2025-08-26T13:54:50.876993Z"
        },
        "id": "70ad5d1517517095"
      },
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "print(train_images.dtype)  # Check the shape of the images tensor\n",
        "print(train_labels.dtype)  # Check the shape of the labels tensor\n",
        "\n",
        "imshow(make_grid(train_images))"
      ],
      "id": "70ad5d1517517095",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.164704Z",
          "start_time": "2025-08-26T13:54:51.162809Z"
        },
        "id": "84521c06cf16fa5f"
      },
      "cell_type": "code",
      "source": [
        "#n_samples / (n_classes * np.bincount(y))\n",
        "\n",
        "#Build my own class weight function\n",
        "\n",
        "def class_weights(data: DataFrame):\n",
        "    weights = []\n",
        "    for col in data.columns[1:]:  # Skip the first column (img_name)\n",
        "        classes = data[col].unique()\n",
        "        total_samples = len(data[col])\n",
        "        class_weight = total_samples / (len(classes) * data[col].value_counts())\n",
        "        weights.append(class_weight[1]) # Assuming the positive class is the second one (1)\n",
        "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "    return class_weights"
      ],
      "id": "84521c06cf16fa5f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.174761Z",
          "start_time": "2025-08-26T13:54:51.171753Z"
        },
        "id": "127afb1be3e34886"
      },
      "cell_type": "code",
      "source": [
        "weights = class_weights(df)\n",
        "print(weights)"
      ],
      "id": "127afb1be3e34886",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.207064Z",
          "start_time": "2025-08-26T13:54:51.182313Z"
        },
        "id": "8d6140e37a73bd72"
      },
      "cell_type": "code",
      "source": [
        "#Applyimg multiclass classification loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=weights).to(device) #Contains softmax and negative log likelihood loss"
      ],
      "id": "8d6140e37a73bd72",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.218279Z",
          "start_time": "2025-08-26T13:54:51.214540Z"
        },
        "id": "1afce429f0732539"
      },
      "cell_type": "code",
      "source": [
        "#https://arxiv.org/pdf/1512.03385\n",
        "#Decided to copy what paper is doing with kernal sizes etc\n",
        "\n",
        "\n",
        "import torch.nn.functional as F #Saves us recreating a RELU instance every single time we wanna use it\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2) # Input channels = 3 (RGB), output channels = 32, kernel size = 5x5, stride = 1, padding = 2 (to keep the spatial dimensions the same)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
        "        #Channels are the amount of filters applied to our spatial data, giving depth to our feature map\n",
        "        #The spatial data will shrink because of 'edge issue' with CNN look at notes for more details\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.avg_global_pool = nn.AdaptiveAvgPool2d(1) # Global average pooling converts our (batch size, num_channels, height width) spatial dimensions into the average per channel across the height and width of the image.\n",
        "        #This makes our shape (batch_size, num_channels, 1,1) containing just the average per each channel across the image (height and width)\n",
        "        #for us its (batch_size, 64, 1,1)\n",
        "        #once flattened this becomes (batch_size, 64 (channels))\n",
        "        #This avoids us needing a fully connected layer with loads of parameters to deal with the large spatial dimensions\n",
        "\n",
        "\n",
        "        self.fc1 = nn.Linear(128, num_classes) #Classification layer (new)\n",
        "        # self.fc1 = nn.Linear(256 * 114 * 114, out_features=len(df.columns) - 1) #Classification layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input x is of shape (batch_size, 3, 512, 512)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        #this function()(x) means, create a new instance of the function and apply it to x\n",
        "        x = F.relu()(x)  # Activation function, done to break the linearity of the model after each convolutional layer\n",
        "\n",
        "        #spatial dimensions: 512\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu()(x)  # Activation function\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #spatial dimensions: 512\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu()(x)  # Activation function\n",
        "\n",
        "        #spatial dimensions: 512\n",
        "\n",
        "        x = self.max_pool(x)  # Max pooling layer, reduces the spatial dimensions by half, you should only pool after a few convolutional layers to reduce the spatial dimensions and retain the features\n",
        "\n",
        "        # 512 / 2 = 256\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu()(x)  # Activation function\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        #spatial dimensions: 256\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu()(x)  # Activation function\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu()(x)  # Activation function\n",
        "\n",
        "        #spatial dimensions: 253\n",
        "\n",
        "        x = self.avg_global_pool(x) # Global average pooling, reduces the spatial dimensions to (batch_size, num_channels, 1, 1) by averaging the height and width dimensions per channel\n",
        "        x = torch.flatten(x, 1)  # Flatten the output to shape (batch_size, num_channels)\n",
        "\n",
        "        x = self.fc1(x)  # Fully connected layer for classification\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_epoch(self, train_loader, loss_values, epoch):\n",
        "        running_loss = 0.0\n",
        "        total_train_data = len(train_loader)\n",
        "        for i, data in enumerate(train_loader):# Start counting from 0, enumerate give us a counter as well as the data\n",
        "            # Every data instance is an input + label pair\n",
        "            inputs, labels = data\n",
        "            #Has to be .float for CrossEntropyLoss\n",
        "            inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # changes label to class indices (1,2,3,4) which is what CrossEntropyLoss wants, instead of one-hot\n",
        "\n",
        "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
        "\n",
        "            outputs = self(inputs) # Make predictions for this batch (forward pass)\n",
        "\n",
        "            loss = criterion(outputs, labels) # Compute the loss and its gradients\n",
        "            loss.backward()  # Backward pass\n",
        "\n",
        "            optimizer.step() # Adjust learning weights\n",
        "            running_loss += loss.item() # running_loss is the total loss for the epoch, we will average it later\n",
        "\n",
        "        avg_loss = running_loss / total_train_data # Average loss for the epoch (over the all batches in the training set, given by len(train_loader))\n",
        "\n",
        "        loss_values.append(avg_loss) #log the average loss for the epoch to the loss_values list\n",
        "        return avg_loss, loss_values"
      ],
      "id": "1afce429f0732539",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A4m01gQqrc-j"
      },
      "id": "A4m01gQqrc-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(4).to(device)  # 4 classes"
      ],
      "metadata": {
        "id": "KKwRICK45FVF"
      },
      "id": "KKwRICK45FVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.227958Z",
          "start_time": "2025-08-26T13:54:51.225083Z"
        },
        "id": "adf1ac72e82f6519"
      },
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-8)"
      ],
      "id": "adf1ac72e82f6519",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.237154Z",
          "start_time": "2025-08-26T13:54:51.235043Z"
        },
        "id": "490b6e8c55ac0c58"
      },
      "cell_type": "code",
      "source": [
        "recent_train_losses = []  # List to keep track of recent training losses for early stopping\n",
        "previous_train_losses = []\n",
        "\n",
        "#This was broken - made a change idk now\n",
        "def early_stopping(avg_loss,patience=5):\n",
        "\n",
        "    if len(recent_train_losses) < patience:\n",
        "        recent_train_losses.append(avg_loss)  # Add the current average loss to the recent losses\n",
        "\n",
        "    if len(previous_train_losses) < patience:\n",
        "        previous_train_losses.append(recent_train_losses[0])\n",
        "        recent_train_losses.pop(0)\n",
        "\n",
        "    if len(recent_train_losses) == patience and len(previous_train_losses) == patience:\n",
        "        avg_of_previous_losses = np.mean(previous_train_losses)\n",
        "        avg_of_recent_previous_losses = np.mean(recent_train_losses)\n",
        "        if avg_of_previous_losses < avg_of_recent_previous_losses:\n",
        "            print(f\"Early stopping condition met: {avg_of_previous_losses} < {avg_of_recent_previous_losses}\")\n",
        "            return True, recent_train_losses, previous_train_losses  # Early stopping condition met\n",
        "\n",
        "        print(f\"Early stopping condition not met: {avg_of_previous_losses} >= {avg_of_recent_previous_losses}\")\n",
        "        previous_train_losses.append(recent_train_losses[0])  #Add oldest recent loss to previous losses\n",
        "        recent_train_losses.pop(0)  # Remove the oldest loss to maintain the size of the list\n",
        "\n",
        "    return False, recent_train_losses, previous_train_losses  # No early stopping condition met\n"
      ],
      "id": "490b6e8c55ac0c58",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:57.642679Z",
          "start_time": "2025-08-26T13:54:51.266832Z"
        },
        "id": "3132729a3bdcb6b9"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#There's a bunch of tensorboard stuff in the pytorch tutorial, we don't need it for now, but we can add it later if we want to visualize the training process\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/classifier_trainer{}'.format(timestamp))\n",
        "\n",
        "EPOCHS = 100\n",
        "\n",
        "best_vloss = 1_000_000. #Super big number to start with, so we can save the model if the validation loss is lower than this\n",
        "\n",
        "# Initialize lists to keep track of loss values for plotting later\n",
        "loss_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "\n",
        "total_val_data = len(val_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch + 1)) # Start counting epochs from 1 for better readability\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data, so we are not in evaluation mode.\n",
        "    model.train(True)\n",
        "\n",
        "    #Actually train the model, this is where the training happens\n",
        "    #Keep track of loss values for plotting later, keep giving the loss_values array to the train_one_epoch function so that it keeps track of the loss values\n",
        "    avg_loss, loss_values = model.train_epoch(train_loader, loss_values,epoch)\n",
        "\n",
        "    early_stop,recent_train_losses, previous_train_losses = early_stopping(avg_loss,patience=5)  # Check if early stopping condition is met\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    #This is the validation loop, it will run after each epoch and compute the validation loss\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(val_loader, 0): #Computes the validation loss per epoch\n",
        "            vinputs, vlabels = vdata\n",
        "            vinputs, vlabels = vinputs.to(device), torch.argmax(vlabels, dim=1).to(device) #changes label to class indices (1,2,3,4) which is what CrossEntropyLoss wants, instead of one-hot\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = criterion(voutputs, vlabels)\n",
        "            #running vloss is the total validation loss for the epoch, we will average it later\n",
        "            running_vloss += vloss.item()  # Add the validation loss for this batch to the running validation loss\n",
        "\n",
        "    # Average validation loss for the epoch (over the all batches in the validation set, given by len(val_loader))\n",
        "    avg_vloss = running_vloss / total_val_data\n",
        "    val_loss_values.append(avg_vloss)\n",
        "\n",
        "    #Gives the loss and validation loss for the epoch into the terminal\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # for both training and validation w]e log the average loss for the epoch\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                       { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                       epoch + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    #Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss: #If the validation loss is lower than the best validation loss so far, save the model\n",
        "\n",
        "        #Best vloss starts at a super high number, so the first model will always be saved\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch + 1) # Save the model with a timestamp and epoch number (epoch + 1 to start from 1)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    if early_stop:  # Check if early stopping condition is met\n",
        "        print(\"Early stopping triggered, stopping training.\")\n",
        "        break  # Stop training if early stopping condition is met\n",
        "\n",
        "    #Onto the next epoch, which will also run through all the batches in the training set and validation set and give the average los s for the epoch"
      ],
      "id": "3132729a3bdcb6b9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e40225ad38b16e4a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Plot the training and validation loss\n",
        "plt.plot(loss_values, label='Training Loss')\n",
        "plt.plot(val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()"
      ],
      "id": "e40225ad38b16e4a"
    },
    {
      "metadata": {
        "id": "7185717e8222c208"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Training set evaluation\n",
        "from torcheval.metrics import MulticlassAccuracy #This lib has so many of these - good to use i think\n",
        "model.eval() #Turns off dropout layers\n",
        "metric = MulticlassAccuracy()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # Move inputs and labels to the same device as the model\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
        "        metric.update(preds, labels)  # Update the metric with predictions and true labels\n",
        "\n",
        "accuracy = metric.compute()  # Compute the accuracy\n",
        "print(f'Training set accuracy: {accuracy:.4f}')  # Print the accuracy"
      ],
      "id": "7185717e8222c208"
    },
    {
      "metadata": {
        "id": "4fb4b141dbb4dbf2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Test set evaluation\n",
        "model.eval()\n",
        "metric = MulticlassAccuracy()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # Move inputs and labels to the same device as the model\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
        "        metric.update(preds, labels)  # Update the metric with predictions and true labels\n",
        "\n",
        "accuracy = metric.compute()  # Compute the accuracy\n",
        "print(f'Test set accuracy: {accuracy:.4f}')  # Print the accuracy"
      ],
      "id": "4fb4b141dbb4dbf2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}