{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "There is an orginal to this file which will have more links and more explanations for the 'first parts' of this file",
   "id": "40f4a81cb002d149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Likely reason this didn't work before is because of imbalance in the dataset - Lets start with location and setting\n",
    "#https://data.csail.mit.edu/graphics/fivek/\n",
    "#https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "#https://www.geeksforgeeks.org/deep-learning/handling-class-imbalance-in-pytorch/\n",
    "#https://docs.pytorch.org/vision/main/generated/torchvision.transforms.ToPILImage.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.utils.class_weight.compute_class_weight.html\n",
    "#https://stackoverflow.com/questions/74020233/how-to-plot-confusion-matrix-in-pytorch\n",
    "#https://docs.pytorch.org/docs/stable/generated/torch.cat.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html\n",
    "#https://stackoverflow.com/questions/46953967/multilabel-indicator-is-not-supported-for-confusion-matrix\n",
    "#https://docs.pytorch.org/vision/stable/transforms.html\n",
    "#https://docs.pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "#https://docs.pytorch.org/docs/stable/data.html\n",
    "#https://docs.pytorch.org/torcheval/main/generated/torcheval.metrics.MultilabelAccuracy.html\n",
    "#https://stackoverflow.com/questions/61524717/pytorch-how-to-find-accuracy-for-multi-label-classification\n",
    "#https://stackoverflow.com/questions/28663856/how-do-i-count-the-occurrence-of-a-certain-item-in-an-ndarray"
   ],
   "id": "9e23543728354d96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install torcheval",
   "id": "132c17ac76697494"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os"
   ],
   "id": "5298bc579a527aba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) #Numpy and Pandas FutureWarnings - ignore them for now, we can fix them later"
   ],
   "id": "a35d1e059c7539ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.read_csv('categories.csv')\n",
    "df.head(10)  # Display the first 10 rows of the dataframe - no print means it will be displayed in the IDE"
   ],
   "id": "c92d38ceff23b05"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6436d6f35b90bb24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Dropping all apart from setting for new mutli-class idea",
   "id": "db0270802baed2ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "df = df.drop(columns=['location','time_of_day','skyCondition']) # Drop other columns, we will use 'setting' as the target variable, input will be the image.\n",
    "df.head(10)"
   ],
   "id": "a0078bedf1260ab4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After checking confusion matrix and the labels predicted less than 20 times, i decided to remove: 'time_of_day_day', 'time_of_day_dusk', 'time_of_day_night', 'time_of_day_unknown', 'skyCondition_artificial', 'skyCondition_mixed', 'skyCondition_sun_sky', location_unknown'\n",
    "\n",
    "this was after did what is stated above, so we can just drop these columns now\n",
    "\n",
    "Check plots/all_features.png for results in confusion matrix"
   ],
   "id": "46d80fa9c87b85eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df = pd.get_dummies(df, columns=['setting'], dtype=\"int\")  # One-hot encode the categorical columns\n",
    "df.columns"
   ],
   "id": "3d9171ff7e1c2cd6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df.drop(['setting_abstract'], axis=1, inplace=True)\n",
    "\n",
    "#Added setting_abstract because there isn't many so the class weight was really high\n",
    "df.head(10)"
   ],
   "id": "1a202ffe74ed7fb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "device = torch.device(\"mps\")",
   "id": "3531904e91c64a5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Now i've organised the files and put them into JPEg forwat with raw_image_converter.py we can have a much simpler and hopefully faster data loading pipeline using torchvision's built in image loading functions",
   "id": "20c268acf0cccba7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.io import decode_image\n",
    "from torchvision.io import ImageReadMode\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, img_dir, transform=None):\n",
    "        self.data = data\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        self.bounds = []\n",
    "\n",
    "        if not self.img_dir:\n",
    "            raise ValueError(f\"No images found in directory: {img_dir}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        img_name = row['img_name']  # Assuming the first column is 'img_name'\n",
    "        #Get the label tensor from the row, excluding the first column (img_name)\n",
    "        label = torch.tensor(row[1:], dtype=torch.long)  # Convert the labels to tensor (excluding img_name, which is the input image name)\n",
    "\n",
    "        img_path = os.path.join(self.img_dir, img_name) + \".jpg\" # Full path to the image file\n",
    "        image = decode_image(img_path, mode=ImageReadMode.RGB) #outputs tensor of shape (C, H, W) where C is the number of channels, H is the height and W is the width\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label # Exclude the first column (img_name) from the label tensor"
   ],
   "id": "5be086ffd1fe8cb7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Transformations for the dataset\n",
    "\n",
    "# These are the standard transformations for the dataset, we will use these for training, testing, and validation\n",
    "val_test_standard_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
    "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
    "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "7a32114b289d14ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Doing data augmentation for training set to get more data and prevent overfitting\n",
    "#These are applied to the training set only, not the validation or test set\n",
    "#They are applied dynamically during training, so sample images will be different each time, effectively increasing the size of the training set\n",
    "\n",
    "\n",
    "#Training transformations with data augmentation\n",
    "training_augmentation_transforms = transforms.Compose([\n",
    "    #Standard transformations for the dataset, we will still use these for training (rest below)\n",
    "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
    "    #Data augmentation transformations for training\n",
    "    # p= probability of applying the transformation, 0.2 means 20% chance of applying the transformation\n",
    "    transforms.RandomHorizontalFlip(p=0.2), # Randomly flip the image horizontally\n",
    "    transforms.RandomVerticalFlip(p=0.2), # Randomly flip the image vertically\n",
    "\n",
    "    #RandomApply makes any transformation randomly applied with a probability p, so we can apply the same transformation with a probability of 0.2\n",
    "\n",
    "    #20% chance of:\n",
    "    transforms.RandomApply(\n",
    "      [\n",
    "          transforms.RandomCrop(size=430, pad_if_needed=True)\n",
    "      ], # Randomly crop the image to 430 in random locations, pad if needed to maintain the size for input to the model (82 pixels on each side, so 82+430+82=512)\n",
    "      p=0.2),\n",
    "\n",
    "\n",
    "    #20% chance of:\n",
    "    transforms.RandomApply(\n",
    "        [\n",
    "            transforms.RandomRotation(30)\n",
    "        ],\n",
    "        p=0.2),\n",
    "\n",
    "      transforms.RandomApply([\n",
    "      transforms.ColorJitter(\n",
    "          brightness=0.3,   # +/- 30% brightness\n",
    "          contrast=0.3,     # +/- 30% contrast\n",
    "          saturation=0.3,   # +/- 30% saturation\n",
    "          hue=0.1           # +/- 0.1 hue shift (~36 degrees)\n",
    "      )\n",
    "  ], p=0.2),\n",
    "\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=0.2), # Randomly adjust the sharpness of the image\n",
    "    transforms.RandomAdjustSharpness(sharpness_factor=0.5,p=0.2), # Randomly adjust the sharpness of the image\n",
    "\n",
    "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
    "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ],
   "id": "dbb1e05e970d56d9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "this is a new method for splitting the dataset into training, test, and validation sets, after i found out train_dataset.dataset and test_dataset.dataset were the same object, so any transformations applied to one would apply to the other, which is not what we want.",
   "id": "adbb69c979722d8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split the dataset into training, validation, and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42) # 70% training, 30% test\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split the test set into validation and test sets\n",
    "#This gives us 70% training, 15% validation, and 15% test sets"
   ],
   "id": "994f0e71849cac42"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Using the new organised images in raw_images_organised with our simpler data loading pipeline using torchvision's built in image loading functions and jpeg format images",
   "id": "12677629fa4db0ff"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_dataset = ImageDataset(train_df, img_dir='raw_images_organised', transform=training_augmentation_transforms)  # Create the training dataset with data augmentation\n",
    "test_dataset = ImageDataset(test_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the test dataset with standard transformations\n",
    "val_dataset = ImageDataset(val_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the validation dataset with standard transformations"
   ],
   "id": "4b323b46cd540fa7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(train_dataset.transform)\n",
    "print(test_dataset.transform)\n",
    "print(val_dataset.transform)"
   ],
   "id": "fa485b2b0d37b71a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create data loaders - these allow us to load images in batches for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ],
   "id": "86ba590ffbf3ef63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.utils import make_grid\n",
    "\n",
    "train_images, train_labels = next(iter(train_loader))\n",
    "print(f\"Train Image Shape: {train_images.shape}\")\n",
    "print(f\"Train Label Shape: {train_labels.shape}\")\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "print(train_images.dtype)  # Check the shape of the images tensor\n",
    "print(train_labels.dtype)  # Check the shape of the labels tensor\n",
    "\n",
    "imshow(make_grid(train_images))"
   ],
   "id": "651a9aeef3bc0522"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Compute class weights to handle class imbalance in the dataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights_pos = []\n",
    "for column in df.columns[1:]:  # Skip the first column (img_name) - Go through each feature and get the class weights to handle class imbalance in dataset for loss function input\n",
    "    class_weight = compute_class_weight(class_weight='balanced',\n",
    "                                        classes=np.unique(df[column]), # Get unique classes in the column (should be 0 and 1 for binary classification) because of one-hot encoding\n",
    "                                        y=df[column]) #y is all the values in the column so we can compute the class weights\n",
    "    print(f\"Class weights for {column}: {class_weight[1]}\") # Print the weight for the positive class (1)\n",
    "    class_weights_pos.append(class_weight[1])  # Append the weight for the positive class (1) for each column\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights_pos, dtype=torch.float32)  # Convert to tensor for use in loss function\n",
    "# Print the class weights\n",
    "print(f\"Class weights: {class_weights_tensor}\")\n",
    "\n",
    "#Higher weight means more rare, so the model will pay more attention to these classes during training"
   ],
   "id": "b3a67bf1c2e214cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Converting to mutli-class",
   "id": "943628c2d2da1d16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor).to(device)",
   "id": "e81c0114ce00eae9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model architecture (better notes in old file model.ipynb)",
   "id": "cf84bfe17757ca6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "77a38f1ec11e5a00"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#https://arxiv.org/pdf/1512.03385\n",
    "#Decided to copy what paper is doing with kernal sizes etc\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2) # Input channels = 3 (RGB), output channels = 32, kernel size = 5x5, stride = 1, padding = 2 (to keep the spatial dimensions the same)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=3, stride=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn5 = nn.BatchNorm2d(64)\n",
    "        self.conv6 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn6 = nn.BatchNorm2d(64)\n",
    "        self.conv7 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "\n",
    "        #Channels are the amount of filters applied to our spatial data, giving depth to our feature map\n",
    "        #The spatial data will shrink because of 'edge issue' with CNN look at notes for more details\n",
    "        self.bn7 = nn.BatchNorm2d(64)\n",
    "\n",
    "\n",
    "\n",
    "        self.avg_global_pool = nn.AdaptiveAvgPool2d(1) # Global average pooling converts our (batch size, num_channels, height width) spatial dimensions into the average per channel across the height and width of the image.\n",
    "        #This makes our shape (batch_size, num_channels, 1,1) containing just the average per each channel across the image (height and width)\n",
    "        #for us its (batch_size, 64, 1,1)\n",
    "        #once flattened this becomes (batch_size, 64 (channels))\n",
    "        #This avoids us needing a fully connected layer with loads of parameters to deal with the large spatial dimensions\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(64, len(df.columns) - 1) #Classification layer (new)\n",
    "        # self.fc1 = nn.Linear(256 * 114 * 114, out_features=len(df.columns) - 1) #Classification layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input x is of shape (batch_size, 3, 512, 512)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        #this function()(x) means, create a new instance of the function and apply it to x\n",
    "        x = nn.ReLU()(x)  # Activation function, done to break the linearity of the model after each convolutional layer\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = nn.ReLU()(x)  # Activation function\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = nn.ReLU()(x)  # Activation function\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = nn.ReLU()(x)  # Activation function\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.max_pool(x)  # Max pooling layer, reduces the spatial dimensions by half, you should only pool after a few convolutional layers to reduce the spatial dimensions and retain the features\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = nn.ReLU()(x)  # Activation function\n",
    "\n",
    "        x = self.conv6(x)\n",
    "        x = self.bn6(x)\n",
    "        x = nn.ReLU()(x)  # Activation function\n",
    "\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "\n",
    "        x = self.avg_global_pool(x) # Global average pooling, reduces the spatial dimensions to (batch_size, num_channels, 1, 1) by averaging the height and width dimensions per channel\n",
    "        x = torch.flatten(x, 1)  # Flatten the output to shape (batch_size, num_channels)\n",
    "\n",
    "        x = self.fc1(x)  # Fully connected layer for classification\n",
    "\n",
    "        return x"
   ],
   "id": "2e556ec9d77a0c23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "net = Model().to(device)",
   "id": "6e5c26d945e36c82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchsummary import summary\n",
    "# summary(net, input_size=(3, 512, 512)) - doesn't work on MPS device, but works on CPU and CUDA"
   ],
   "id": "aa375c5ae59439bb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Weight_decay sets and enables L2 regularization in the cost function, it adds a penalty to the loss function based on the magnitude of the weights, which helps prevent overfitting by discouraging large weights. 1e-5 is the resNet default.\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001, weight_decay=1e-6)  # Using Adam optimizer with a learning rate of 0.001"
   ],
   "id": "85e83a1fc61ec2ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#There's a bunch of tensorboard stuff in the pytorch tutorial, we don't need it for now, but we can add it later if we want to visualize the training process\n",
    "\n",
    "#This function gets called once per epoch in loop below\n",
    "def train_one_epoch(train_loader,loss_values):\n",
    "\n",
    "    running_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    total_train_data = len(train_loader)\n",
    "    for i, data in enumerate(train_loader, 0):  # Start counting from 0, enumerate give us a counter as well as the data\n",
    "        # Every data instance is an input + label pair\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float()  # Move inputs and labels to the same device as the model (apple silicon chip, cuda, or cpu)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = net(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # running_loss is the total loss for the epoch, we will average it later\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / total_train_data  # Average loss for the epoch (over the all batches in the training set, given by len(train_loader))\n",
    "\n",
    "    # Log the average loss for the epoch:\n",
    "    loss_values.append(avg_loss) #log the average loss for the epoch to the loss_values list\n",
    "    # Log the average loss for the epoch\n",
    "    return avg_loss, loss_values"
   ],
   "id": "8b240e686a369ac1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize list and int to keep track of recent training losses and average losses for early stopping\n",
    "\n",
    "# avg_of_recent_losses = 0.0 #Doesn't work because we cannot reasign a value within our function due to scope, so we will return it from the function\n",
    "\n",
    "#Early stopping function to stop training if the validation loss doesn't improve for a certain number of epochs (patience)\n",
    "# Initialize a list to keep track of recent training losses for early stopping\n",
    "recent_train_losses = []  # List to keep track of recent training losses for early stopping\n",
    "previous_train_losses = []\n",
    "\n",
    "#This was broken - made a change idk now\n",
    "def early_stopping(avg_loss,patience=5):\n",
    "\n",
    "    if len(recent_train_losses) < patience:\n",
    "        recent_train_losses.append(avg_loss)  # Add the current average loss to the recent losses\n",
    "\n",
    "    if len(previous_train_losses) < patience:\n",
    "        previous_train_losses.append(recent_train_losses[0])\n",
    "        recent_train_losses.pop(0)\n",
    "\n",
    "    if len(recent_train_losses) == patience and len(previous_train_losses) == patience:\n",
    "        avg_of_previous_losses = np.mean(previous_train_losses)\n",
    "        avg_of_recent_previous_losses = np.mean(recent_train_losses)\n",
    "        if avg_of_previous_losses < avg_of_recent_previous_losses:\n",
    "            print(f\"Early stopping condition met: {avg_of_previous_losses} < {avg_of_recent_previous_losses}\")\n",
    "            return True, recent_train_losses, previous_train_losses  # Early stopping condition met\n",
    "\n",
    "        print(f\"Early stopping condition not met: {avg_of_previous_losses} >= {avg_of_recent_previous_losses}\")\n",
    "        previous_train_losses.append(recent_train_losses[0])  #Add oldest recent loss to previous losses\n",
    "        recent_train_losses.pop(0)  # Remove the oldest loss to maintain the size of the list\n",
    "\n",
    "    return False, recent_train_losses, previous_train_losses  # No early stopping condition met\n"
   ],
   "id": "c7f0c165322b2e43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#There's a bunch of tensorboard stuff in the pytorch tutorial, we don't need it for now, but we can add it later if we want to visualize the training process\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/classifier_trainer{}'.format(timestamp))\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "best_vloss = 1_000_000. #Super big number to start with, so we can save the model if the validation loss is lower than this\n",
    "\n",
    "# Initialize lists to keep track of loss values for plotting later\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "\n",
    "total_val_data = len(val_loader)\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1)) # Start counting epochs from 1 for better readability\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data, so we are not in evaluation mode.\n",
    "    net.train(True)\n",
    "\n",
    "    #Actually train the model, this is where the training happens\n",
    "    #Keep track of loss values for plotting later, keep giving the loss_values array to the train_one_epoch function so that it keeps track of the loss values\n",
    "    avg_loss, loss_values = train_one_epoch(train_loader, loss_values)\n",
    "\n",
    "    early_stop,recent_train_losses, previous_train_losses = early_stopping(avg_loss,patience=5)  # Check if early stopping condition is met\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    net.eval()\n",
    "\n",
    "    #This is the validation loop, it will run after each epoch and compute the validation loss\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(val_loader, 0): #Computes the validation loss per epoch\n",
    "            vinputs, vlabels = vdata\n",
    "            vinputs, vlabels = vinputs.to(device), vlabels.to(device).float()  # Move inputs and labels to the same device as the model\n",
    "            voutputs = net(vinputs)\n",
    "            vloss = criterion(voutputs, vlabels)\n",
    "            #running vloss is the total validation loss for the epoch, we will average it later\n",
    "            running_vloss += vloss.item()  # Add the validation loss for this batch to the running validation loss\n",
    "\n",
    "    # Average validation loss for the epoch (over the all batches in the validation set, given by len(val_loader))\n",
    "    avg_vloss = running_vloss / total_val_data\n",
    "    val_loss_values.append(avg_vloss)\n",
    "\n",
    "    #Gives the loss and validation loss for the epoch into the terminal\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # for both training and validation w]e log the average loss for the epoch\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                       { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                       epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    #Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss: #If the validation loss is lower than the best validation loss so far, save the model\n",
    "\n",
    "        #Best vloss starts at a super high number, so the first model will always be saved\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch + 1) # Save the model with a timestamp and epoch number (epoch + 1 to start from 1)\n",
    "        torch.save(net.state_dict(), model_path)\n",
    "\n",
    "    if early_stop:  # Check if early stopping condition is met\n",
    "        print(\"Early stopping triggered, stopping training.\")\n",
    "        break  # Stop training if early stopping condition is met\n",
    "\n",
    "    #Onto the next epoch, which will also run through all the batches in the training set and validation set and give the average los s for the epoch"
   ],
   "id": "c41fae272f9d4f08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# #Resplit dataset for cross validation\n",
    "# train_size = int(0.8 * len(dataset))  # 80% for training\n",
    "# test_size = len(dataset) - train_size  # Remaining 20% for testing\n",
    "# train_dataset, test_dataset = random_split(dataset, [train_size, test_size])  # Split the dataset into training and test sets"
   ],
   "id": "5ed40aef7e14a997"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below isn't used it was an attempt at cross validation",
   "id": "5cde976fec6107a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Removed it was cross validation code but it was shit",
   "id": "130a994bd82159ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot the training and validation loss\n",
    "plt.plot(loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()"
   ],
   "id": "7566fd4ed8a86802"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# net = Model().to(device) # Create a new instance of the model architecture but still with GPU support\n",
    "# net.load_state_dict(torch.load('model_20250818_204816_43')) #Only imports the weights, not the model architecture\n",
    "# net = net.to(device) # Create a new instance of the model architecture but still with GPU support"
   ],
   "id": "a348a9d75f348201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Training set evaluation\n",
    "from torcheval.metrics import MulticlassAccuracy #This lib has so many of these - good to use i think\n",
    "model.eval() #Turns off dropout layers\n",
    "metric = MulticlassAccuracy()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
    "        metric.update(preds, labels)  # Update the metric with predictions and true labels\n",
    "\n",
    "accuracy = metric.compute()  # Compute the accuracy\n",
    "print(f'Training set accuracy: {accuracy:.4f}')  # Print the accuracy"
   ],
   "id": "15bb925b91bd8698"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "id": "b1bb7231869438ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Test set evaluation\n",
    "model.eval()\n",
    "metric = MulticlassAccuracy()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
    "        metric.update(preds, labels)  # Update the metric with predictions and true labels\n",
    "\n",
    "accuracy = metric.compute()  # Compute the accuracy\n",
    "print(f'Test set accuracy: {accuracy:.4f}')  # Print the accuracy"
   ],
   "id": "16d1a584aacb9783"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "#Looks like we have overfitting, fix this, attempting dropout regularization - done",
   "id": "f8cb5e6a8a1e44a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Below is prep for the confusion matrix plotted 2 cells below, this is where we get the predictions and labels from the test set evaluation above, so we can plot the confusion matrix and prepare the data for it.",
   "id": "18ee642dcd53be5b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Plot a confusion matrix (prepping data for it)\n",
    "#https://stackoverflow.com/questions/46953967/multilabel-indicator-is-not-supported-for-confusion-matrix\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\n",
    "# Concatenate all predictions and labels into a single tensor (currently they are lists of tensors)\n",
    "\n",
    "#Basically by counting the amount of each class number that appear in this tensor, we can get the confusion matrix  (cell below)\n",
    "all_predictions_tensor = torch.cat(predictions, dim=0) # Concatenate all predictions that appeared in the test set into a single tensor\n",
    "all_labels_tensor = torch.cat(labels_list, dim=0)  # Concatenate all labels that appeared in the test set into a single tensor\n",
    "\n",
    "\n",
    "# Convert to CPU tensors then to numpy arrays for confusion matrix\n",
    "all_predictions_tensor = all_predictions_tensor.cpu().numpy()\n",
    "all_labels_tensor = all_labels_tensor.cpu().numpy()\n",
    "print(all_predictions_tensor.shape, all_labels_tensor.shape)  # Check the shapes of the tensors\n",
    "print(all_predictions_tensor[0].shape, all_labels_tensor[0].shape) # Check the shape of the first prediction and label tensors\n",
    "\n",
    "\n",
    "#Data datatype before argmax, they are one-hot encoded tensors (boolean tensors) no good for confusion matrix\n",
    "print(all_predictions_tensor.dtype, all_predictions_tensor.dtype)\n",
    "\n",
    "# Argmax to convert one-hot encoded predictions and labels to a list of predicted and actual labels as integers\n",
    "all_predictions_tensor = all_predictions_tensor.argmax(axis=1)\n",
    "all_labels_tensor = all_labels_tensor.argmax(axis=1)\n",
    "print(all_predictions_tensor.dtype, all_labels_tensor.dtype)\n",
    "\n",
    "\n",
    "print(np.unique(all_predictions_tensor))\n",
    "print(np.unique(all_labels_tensor)) #Aah, there were only 7 unique labels in the test set\n",
    "# so we need to make sure the confusion matrix doesn't drop the 'empty labels' because the model might still have predicted them, but it could have been wrong\n"
   ],
   "id": "4ae75873eebb8a31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "display_labels = df.columns[1:]  # Exclude the first column (img_name) for display labels\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels_tensor, all_predictions_tensor, labels=np.arange(len(display_labels)))\n",
    "#np.arrange basically just creates an array of length of display_labels (array of 15 elements, 0 to 14) which are the labels for the possible labels in the dataset, so the confusion matrix will have 15 rows and 15 columns, one for each label\n",
    "\n",
    "#Without display_labels the confusion matrix would only render the labels that were actually predicted, so if a label was never predicted, it would not appear in the confusion matrix, which is not what we want\n",
    "\n",
    "#15 labels, so 15x15 matrix\n",
    "print(cm.shape)\n",
    "\n",
    "matrix = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels) #disiplay_labels are the text labels (from column names) for the confusion matrix, these are the labels we want to display on the x and y axis of the confusion matrix, these are the same order as the labels we used for confusion matrix computation above,thanks to np.arrange(), so they will line up correctly\n",
    "\n",
    "_, ax = plt.subplots(figsize=(10, 10))  # Create a figure and axis for the confusion matrix plot (much larger than default\n",
    "matrix.plot(cmap='Blues', values_format='d', ax=ax, xticks_rotation=90)  # Plot the confusion matrix with blue color map and integer format\n"
   ],
   "id": "a129d16747a1d6a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Keep in mind this matrix is from the test set\n",
    "\n",
    "Only parts of matrix are useful because we are doing multi-label classification, basically just look at the related features from the one-hot.\n",
    "\n",
    "So for location this would be: True label: location_outdoor, Predicted label: location_indoor or location_outdoor, etc. no point looking at the unrelated features like setting when trying to see how well the model predicts location.\n",
    "\n",
    "Matrix shows you when the model isn't choosing any labels from a grouping like location, its not choosing any of the one-hot encoded location labels. where as setting, seems much more 'active' in the matrix, meaning the model is choosing labels from that grouping more often."
   ],
   "id": "b290bf4031e78948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#Lets get all the labels that were predicted less than 10 times in the test set, so we can see which labels are not being predicted at all\n",
    "predicted_counts = np.bincount(all_predictions_tensor, minlength=len(display_labels))  # Count the occurrences of each predicted label, minlength ensures we count all labels even if some were not predicted at all\n",
    "\n",
    "unpredicted_labels = [label for label, count in zip(display_labels, predicted_counts) if count < 20]  # Get labels that were predicted less than 10 times, zip pairs each label with its count, so we can filter them based on the count, this works because they are the same length (thanks to minlenth) and in the same order.\n",
    "\n",
    "\n",
    "print(\"Labels predicted less than 10 times:\", unpredicted_labels)  # Print the labels that were predicted less than 20 times\n",
    "\n",
    "#These are the labels that our model can't do so will be removed, likely due to the data imbalance in the dataset, so we will remove them from the dataset and retrain the model"
   ],
   "id": "d51c0d2483bf0163"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "After checking the confusion matrix and the labels predicted less than 20 times, i decided to remove: 'time_of_day_day', 'time_of_day_dusk', 'time_of_day_night', 'time_of_day_unknown', 'skyCondition_artificial', 'skyCondition_mixed', 'skyCondition_sun_sky', location_unknown'",
   "id": "feabc7505272df74"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "torch.save(net.state_dict(), \"final_model_mutlilabel.pth\")  # Save the final model state dictionary",
   "id": "9ee4d310358002ea"
  }
 ],
 "metadata": {},
 "nbformat": 5,
 "nbformat_minor": 9
}
