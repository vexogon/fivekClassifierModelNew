{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vexogon/fivekClassifierModelNew/blob/main/Working%20Models/Multi%20Class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:49.884037Z",
          "start_time": "2025-08-26T13:54:48.749030Z"
        },
        "id": "initial_id"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from pandas import DataFrame\n",
        "from torch import nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os"
      ],
      "outputs": [],
      "execution_count": 68
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.330009Z",
          "start_time": "2025-08-26T13:54:49.886600Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f98121cbedb5d9d3",
        "outputId": "4f7c728c-ad9b-4bf5-bea6-fff78d3e4917"
      },
      "cell_type": "code",
      "source": [
        "!pip install torcheval"
      ],
      "id": "f98121cbedb5d9d3",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from torcheval) (4.15.0)\n"
          ]
        }
      ],
      "execution_count": 69
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.385264Z",
          "start_time": "2025-08-26T13:54:50.383620Z"
        },
        "id": "e4c34c6c0e28ed53"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) #Numpy and Pandas FutureWarnings - ignore them for now, we can fix them later"
      ],
      "id": "e4c34c6c0e28ed53",
      "outputs": [],
      "execution_count": 70
    },
    {
      "metadata": {
        "id": "fc70bb11ff9bde24"
      },
      "cell_type": "markdown",
      "source": [
        "Reading in our CSV file, and doing some preprocessing to get it ready for training"
      ],
      "id": "fc70bb11ff9bde24"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.399351Z",
          "start_time": "2025-08-26T13:54:50.391464Z"
        },
        "id": "f09f9b8a6f72952b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "2cc2fedc-54c8-436d-8b1f-fe6b00b3ce2a"
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('categories.csv')\n",
        "df.head()"
      ],
      "id": "f09f9b8a6f72952b",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   img_name location time_of_day skyCondition   setting\n",
              "0        a0001-jmac_DSC1459  outdoor         day      sun_sky    nature\n",
              "1             a0002-dgw_005   indoor         day        mixed    people\n",
              "2        a0003-NKIM_MG_8178  outdoor         day      sun_sky  man_made\n",
              "3        a0004-jmac_MG_1384  outdoor         day      sun_sky    nature\n",
              "4  a0005-jn_2007_05_10__564  outdoor         day      sun_sky  man_made"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a5fa1c6c-d5f2-430c-ac1d-7d20d46c71cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>location</th>\n",
              "      <th>time_of_day</th>\n",
              "      <th>skyCondition</th>\n",
              "      <th>setting</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a0001-jmac_DSC1459</td>\n",
              "      <td>outdoor</td>\n",
              "      <td>day</td>\n",
              "      <td>sun_sky</td>\n",
              "      <td>nature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a0002-dgw_005</td>\n",
              "      <td>indoor</td>\n",
              "      <td>day</td>\n",
              "      <td>mixed</td>\n",
              "      <td>people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a0003-NKIM_MG_8178</td>\n",
              "      <td>outdoor</td>\n",
              "      <td>day</td>\n",
              "      <td>sun_sky</td>\n",
              "      <td>man_made</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a0004-jmac_MG_1384</td>\n",
              "      <td>outdoor</td>\n",
              "      <td>day</td>\n",
              "      <td>sun_sky</td>\n",
              "      <td>nature</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a0005-jn_2007_05_10__564</td>\n",
              "      <td>outdoor</td>\n",
              "      <td>day</td>\n",
              "      <td>sun_sky</td>\n",
              "      <td>man_made</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a5fa1c6c-d5f2-430c-ac1d-7d20d46c71cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a5fa1c6c-d5f2-430c-ac1d-7d20d46c71cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a5fa1c6c-d5f2-430c-ac1d-7d20d46c71cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-025f2014-080a-45df-90fa-193b6fbbc23c\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-025f2014-080a-45df-90fa-193b6fbbc23c')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-025f2014-080a-45df-90fa-193b6fbbc23c button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"img_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"a1502-NKIM_MG_6279\",\n          \"a2587-LSCRW_0047\",\n          \"a2654-IMG_0032\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"outdoor\",\n          \"indoor\",\n          \"unknown\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_of_day\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"unknown\",\n          \"night\",\n          \"day\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"skyCondition\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"sun_sky\",\n          \"mixed\",\n          \"artificial\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"people\",\n          \"abstract\",\n          \"man_made\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "execution_count": 71
    },
    {
      "metadata": {
        "id": "53521c73a882108c"
      },
      "cell_type": "markdown",
      "source": [
        "Lets drop all the columns that aren't one for our classes for multilabel classification, so thats location, time_of_day, skyCondition"
      ],
      "id": "53521c73a882108c"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.417889Z",
          "start_time": "2025-08-26T13:54:50.416003Z"
        },
        "id": "b3936f1f300a78f4"
      },
      "cell_type": "code",
      "source": [
        "df = df.drop(columns=['location','time_of_day','skyCondition'])"
      ],
      "id": "b3936f1f300a78f4",
      "outputs": [],
      "execution_count": 72
    },
    {
      "metadata": {
        "id": "214f106b1d84c5f2"
      },
      "cell_type": "markdown",
      "source": [
        "Nows lets one-hot encode the 'setting' column, so we have a column for each setting type, with a 1 if the image is of that setting type, and a 0 if it isn't"
      ],
      "id": "214f106b1d84c5f2"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.440947Z",
          "start_time": "2025-08-26T13:54:50.437464Z"
        },
        "id": "8d988beb92680e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b5b439-e73a-4ef2-dfc4-5232171cf923"
      },
      "cell_type": "code",
      "source": [
        "df = pd.get_dummies(df, columns=['setting'], dtype='long')\n",
        "df.columns"
      ],
      "id": "8d988beb92680e2d",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['img_name', 'setting_abstract', 'setting_animals', 'setting_man_made',\n",
              "       'setting_nature', 'setting_people'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "execution_count": 73
    },
    {
      "metadata": {
        "id": "4639ca684260eeb1"
      },
      "cell_type": "markdown",
      "source": [
        "Removing the 'setting_abstract' column, as it doesn't have enough examples to be useful for training"
      ],
      "id": "4639ca684260eeb1"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.463706Z",
          "start_time": "2025-08-26T13:54:50.460386Z"
        },
        "id": "585dc337dc595946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a906b70d-b3a2-4ff1-c90c-feb2b87a8bf5"
      },
      "cell_type": "code",
      "source": [
        "df.drop('setting_abstract', axis=1, inplace=True) #Abstract doesn't have enough examples\n",
        "df.head()"
      ],
      "id": "585dc337dc595946",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   img_name  setting_animals  setting_man_made  \\\n",
              "0        a0001-jmac_DSC1459                0                 0   \n",
              "1             a0002-dgw_005                0                 0   \n",
              "2        a0003-NKIM_MG_8178                0                 1   \n",
              "3        a0004-jmac_MG_1384                0                 0   \n",
              "4  a0005-jn_2007_05_10__564                0                 1   \n",
              "\n",
              "   setting_nature  setting_people  \n",
              "0               1               0  \n",
              "1               0               1  \n",
              "2               0               0  \n",
              "3               1               0  \n",
              "4               0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f50ae8f-f0d9-49e1-a4fb-ddd0e7d2b2b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>img_name</th>\n",
              "      <th>setting_animals</th>\n",
              "      <th>setting_man_made</th>\n",
              "      <th>setting_nature</th>\n",
              "      <th>setting_people</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a0001-jmac_DSC1459</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a0002-dgw_005</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>a0003-NKIM_MG_8178</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>a0004-jmac_MG_1384</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a0005-jn_2007_05_10__564</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f50ae8f-f0d9-49e1-a4fb-ddd0e7d2b2b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3f50ae8f-f0d9-49e1-a4fb-ddd0e7d2b2b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3f50ae8f-f0d9-49e1-a4fb-ddd0e7d2b2b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-b4875bee-ea62-45af-bdc7-5b716fbfd8c6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b4875bee-ea62-45af-bdc7-5b716fbfd8c6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-b4875bee-ea62-45af-bdc7-5b716fbfd8c6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"img_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"a1502-NKIM_MG_6279\",\n          \"a2587-LSCRW_0047\",\n          \"a2654-IMG_0032\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting_animals\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting_man_made\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting_nature\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"setting_people\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "execution_count": 74
    },
    {
      "metadata": {
        "id": "39ba1cd3fb0cdb00"
      },
      "cell_type": "markdown",
      "source": [
        "Removing any rows without a one-hot encoded class, as they will distract the model during training"
      ],
      "id": "39ba1cd3fb0cdb00"
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove any rows without any class\n",
        "filtered_df = df[(df == 1).any(axis=1)]\n",
        "print(filtered_df)\n",
        "df = filtered_df #Set as main dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMAx1o5Ow8FV",
        "outputId": "88587e81-f49c-4474-b464-f92013b5cfa8"
      },
      "id": "WMAx1o5Ow8FV",
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                      img_name  setting_animals  setting_man_made  \\\n",
            "0           a0001-jmac_DSC1459                0                 0   \n",
            "1                a0002-dgw_005                0                 0   \n",
            "2           a0003-NKIM_MG_8178                0                 1   \n",
            "3           a0004-jmac_MG_1384                0                 0   \n",
            "4     a0005-jn_2007_05_10__564                0                 1   \n",
            "...                        ...              ...               ...   \n",
            "4994           a4995-kme_00069                0                 1   \n",
            "4995  a4996-Duggan_090426_7783                0                 1   \n",
            "4996            a4997-kme_0558                0                 0   \n",
            "4998            a4999-DSC_0035                0                 1   \n",
            "4999            a5000-kme_0204                0                 0   \n",
            "\n",
            "      setting_nature  setting_people  \n",
            "0                  1               0  \n",
            "1                  0               1  \n",
            "2                  0               0  \n",
            "3                  1               0  \n",
            "4                  0               0  \n",
            "...              ...             ...  \n",
            "4994               0               0  \n",
            "4995               0               0  \n",
            "4996               0               1  \n",
            "4998               0               0  \n",
            "4999               1               0  \n",
            "\n",
            "[4599 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "e025f24180d7f313"
      },
      "cell_type": "markdown",
      "source": [
        "Setting the device to use for training, in this case a GPU if available"
      ],
      "id": "e025f24180d7f313"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.503601Z",
          "start_time": "2025-08-26T13:54:50.502240Z"
        },
        "id": "a43fdf82a440a400"
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "id": "a43fdf82a440a400",
      "outputs": [],
      "execution_count": 76
    },
    {
      "metadata": {
        "id": "6a549de192fafad5"
      },
      "cell_type": "markdown",
      "source": [
        "Our custom dataset class, which will read in the images and labels from the CSV file and image directory, this is greatly simplified compared with the raw image version in earlier notebooks, check model.ipynb for that"
      ],
      "id": "6a549de192fafad5"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.535706Z",
          "start_time": "2025-08-26T13:54:50.533510Z"
        },
        "id": "80e3e585a420b8b"
      },
      "cell_type": "code",
      "source": [
        "from torchvision.io import decode_image\n",
        "from torchvision.io import ImageReadMode\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data, img_dir, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        self.img_dir = img_dir\n",
        "        self.bounds = []\n",
        "\n",
        "        if not self.img_dir:\n",
        "            raise ValueError(f\"No images found in directory: {img_dir}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_name = row['img_name']  # Assuming the first column is 'img_name'\n",
        "        #Get the label tensor from the row, excluding the first column (img_name)\n",
        "        label = torch.tensor(row[1:], dtype=torch.long)  # Convert the labels to tensor (excluding img_name, which is the input image name)\n",
        "\n",
        "        img_path = os.path.join(self.img_dir, img_name) + \".jpg\" # Full path to the image file\n",
        "        image = decode_image(img_path, mode=ImageReadMode.RGB) #outputs tensor of shape (C, H, W) where C is the number of channels, H is the height and W is the width\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "id": "80e3e585a420b8b",
      "outputs": [],
      "execution_count": 77
    },
    {
      "metadata": {
        "id": "ebdb5e6a29981de3"
      },
      "cell_type": "markdown",
      "source": [
        "Standard transformations for the dataset, and data augmentation for the training set"
      ],
      "id": "ebdb5e6a29981de3"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.563585Z",
          "start_time": "2025-08-26T13:54:50.561231Z"
        },
        "id": "6ccd55e4ef8b145f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab11d078-cacf-418d-87bc-7efcf4df7cdf"
      },
      "cell_type": "code",
      "source": [
        "# Transformations for the dataset\n",
        "\n",
        "# These are the standard transformations for the dataset, we will use these for training, testing, and validation\n",
        "val_test_standard_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
        "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
        "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "id": "6ccd55e4ef8b145f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 78
    },
    {
      "metadata": {
        "id": "2f01902829b07c6d"
      },
      "cell_type": "markdown",
      "source": [
        "This is our data augmentation for the training set, to get more unique data and prevent overfitting, these transformations are applied dynamically during training, so sample images will be different each time, effectively increasing the size of the training set"
      ],
      "id": "2f01902829b07c6d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.591514Z",
          "start_time": "2025-08-26T13:54:50.589087Z"
        },
        "id": "71d5e6ce15be89b1",
        "outputId": "08e9290a-3f30-417f-c21d-6026350541e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "cell_type": "code",
      "source": [
        "#Doing data augmentation for training set to get more data and prevent overfitting\n",
        "#These are applied to the training set only, not the validation or test set\n",
        "#They are applied dynamically during training, so sample images will be different each time, effectively increasing the size of the training set\n",
        "\n",
        "\n",
        "#Training transformations with data augmentation\n",
        "training_augmentation_transforms = transforms.Compose([\n",
        "    #Standard transformations for the dataset, we will still use these for training (rest below)\n",
        "    transforms.ToPILImage(), # Convert numpy array to PIL Image\n",
        "    #Data augmentation transformations for training\n",
        "    # p= probability of applying the transformation, 0.2 means 20% chance of applying the transformation\n",
        "\n",
        "    #RandomApply makes any transformation randomly applied with a probability p, so we can apply the same transformation with a probability of 0.2\n",
        "\n",
        "    #50% chance of:\n",
        "    transforms.RandomApply(\n",
        "      [\n",
        "          transforms.RandomCrop(size=430, pad_if_needed=True)\n",
        "      ], # Randomly crop the image to 430 in random locations, pad if needed to maintain the size for input to the model (82 pixels on each side, so 82+430+82=512)\n",
        "      p=0.5),\n",
        "\n",
        "\n",
        "    #50% chance of:\n",
        "    transforms.RandomApply(\n",
        "        [\n",
        "            transforms.RandomRotation(30)\n",
        "        ],\n",
        "        p=0.5),\n",
        "\n",
        "    #50% chance of:\n",
        "      transforms.RandomApply([\n",
        "      transforms.ColorJitter(\n",
        "          brightness=0.3,   # +/- 30% brightness\n",
        "          contrast=0.3,     # +/- 30% contrast\n",
        "          saturation=0.3,   # +/- 30% saturation\n",
        "          hue=0.1           # +/- 0.1 hue shift (~36 degrees)\n",
        "      )\n",
        "  ], p=0.5),\n",
        "\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2,p=0.4), # Randomly adjust the sharpness of the image\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=0.5,p=0.4), # Randomly adjust the sharpness of the image\n",
        "\n",
        "    transforms.Resize((512, 512)), # Resize the image to 512x512 for consistency and to match the model input size\n",
        "    transforms.ToTensor(), # Convert PIL Image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "id": "71d5e6ce15be89b1",
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-1614986758.py, line 40)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-1614986758.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    transforms.RandomGrayscale(p=0.2)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ],
      "execution_count": 79
    },
    {
      "metadata": {
        "id": "828fb014740607ba"
      },
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into training, validation, and test sets, 70% training, 15% validation, 15% test, in order to get transformations applied dynamically during training, we need to create the datasets after the split"
      ],
      "id": "828fb014740607ba"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.738423Z",
          "start_time": "2025-08-26T13:54:50.601156Z"
        },
        "id": "d530f2890ee254e3"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the dataset into training, validation, and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42) # 70% training, 30% test\n",
        "val_df, test_df = train_test_split(test_df, test_size=0.5, random_state=42)  # Split the test set into validation and test sets\n",
        "#This gives us 70% training, 15% validation, and 15% test sets"
      ],
      "id": "d530f2890ee254e3",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "59b21ec2be329e7"
      },
      "cell_type": "markdown",
      "source": [
        "Creating the datasets with the appropriate transformations using train_df val_df and test_df"
      ],
      "id": "59b21ec2be329e7"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.750202Z",
          "start_time": "2025-08-26T13:54:50.748546Z"
        },
        "id": "37259b001bc7523c"
      },
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(train_df, img_dir='raw_images_organised', transform=training_augmentation_transforms)  # Create the training dataset with data augmentation\n",
        "test_dataset = ImageDataset(test_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the test dataset with standard transformations\n",
        "val_dataset = ImageDataset(val_df, img_dir='raw_images_organised', transform=val_test_standard_transforms)  # Create the validation dataset with standard transformations"
      ],
      "id": "37259b001bc7523c",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.754692Z",
          "start_time": "2025-08-26T13:54:50.753170Z"
        },
        "id": "86ad59fab4f69ccc"
      },
      "cell_type": "code",
      "source": [
        "print(train_dataset.transform)\n",
        "print(test_dataset.transform)\n",
        "print(val_dataset.transform)"
      ],
      "id": "86ad59fab4f69ccc",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "2499c30687e87a61"
      },
      "cell_type": "markdown",
      "source": [
        "These will produce the batches for training, validation, and testing, shuffling the training data to prevent overfitting. These are what creates that shape (batch_size, channels, height, width) for the model input. in order case of (32, 3, 512, 512) for a batch size of 32, 3 channels (RGB), and 512x512 image size"
      ],
      "id": "2499c30687e87a61"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.761763Z",
          "start_time": "2025-08-26T13:54:50.759917Z"
        },
        "id": "c0c0ba7631217bf1"
      },
      "cell_type": "code",
      "source": [
        "#Data loaders for batching and shuffling the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "id": "c0c0ba7631217bf1",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:50.868179Z",
          "start_time": "2025-08-26T13:54:50.766608Z"
        },
        "id": "a72726019a18c8ed"
      },
      "cell_type": "code",
      "source": [
        "train_images, train_labels = next(iter(train_loader))\n",
        "print(train_images.shape, train_labels.shape)"
      ],
      "id": "a72726019a18c8ed",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "19794fe2f6c9090a"
      },
      "cell_type": "markdown",
      "source": [
        "Rendering some sample images from the training set to check that the data augmentation is working as expected"
      ],
      "id": "19794fe2f6c9090a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.153624Z",
          "start_time": "2025-08-26T13:54:50.876993Z"
        },
        "id": "70ad5d1517517095"
      },
      "cell_type": "code",
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "print(train_images.dtype)  # Check the shape of the images tensor\n",
        "print(train_labels.dtype)  # Check the shape of the labels tensor\n",
        "\n",
        "imshow(make_grid(train_images))"
      ],
      "id": "70ad5d1517517095",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "13b259acd2befb26"
      },
      "cell_type": "markdown",
      "source": [
        "Class weights to handle class imbalance, using custom function based on sklearn's compute_class_weight function"
      ],
      "id": "13b259acd2befb26"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.164704Z",
          "start_time": "2025-08-26T13:54:51.162809Z"
        },
        "id": "84521c06cf16fa5f"
      },
      "cell_type": "code",
      "source": [
        "#n_samples / (n_classes * np.bincount(y))\n",
        "\n",
        "#Build my own class weight function\n",
        "\n",
        "def class_weights(data: DataFrame):\n",
        "    weights = []\n",
        "    for col in data.columns[1:]:  # Skip the first column (img_name)\n",
        "        classes = data[col].unique()\n",
        "        total_samples = len(data[col])\n",
        "        class_weight = total_samples / (len(classes) * data[col].value_counts())\n",
        "        weights.append(class_weight[1]) # Assuming the positive class is the second one (1)\n",
        "    class_weights = torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "    return class_weights"
      ],
      "id": "84521c06cf16fa5f",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.174761Z",
          "start_time": "2025-08-26T13:54:51.171753Z"
        },
        "id": "127afb1be3e34886"
      },
      "cell_type": "code",
      "source": [
        "weights = class_weights(df)\n",
        "print(weights)"
      ],
      "id": "127afb1be3e34886",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "6d125e2e6677262a"
      },
      "cell_type": "markdown",
      "source": [
        "Cross entropy loss function for multiclass classification, with class weights to handle class imbalance, and moving it to the appropriate device (GPU if available). this cost function combines softmax and negative log likelihood loss, so we don't need to apply softmax to the model outputs before passing them to the loss function"
      ],
      "id": "6d125e2e6677262a"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.207064Z",
          "start_time": "2025-08-26T13:54:51.182313Z"
        },
        "id": "8d6140e37a73bd72"
      },
      "cell_type": "code",
      "source": [
        "#Applyimg multiclass classification loss function\n",
        "criterion = nn.CrossEntropyLoss(weight=weights).to(device) #Contains softmax and negative log likelihood loss"
      ],
      "id": "8d6140e37a73bd72",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "58dbbb268934155d"
      },
      "cell_type": "markdown",
      "source": [
        "Our model architecture, a CNN with 9 convolutional layers, batch normalization, dropout, max pooling, and global average pooling before the final fully connected layer for classification. Using ReLU activation functions after each convolutional layer to introduce non-linearity. The model is designed to take in 512x512 RGB images and output class scores for each of the classes in the dataset. These can then have argmax applied to get the predicted class for each image, creating class indices (0,1,2,3) which is what CrossEntropyLoss wants, instead of one-hot"
      ],
      "id": "58dbbb268934155d"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.218279Z",
          "start_time": "2025-08-26T13:54:51.214540Z"
        },
        "id": "1afce429f0732539"
      },
      "cell_type": "code",
      "source": [
        "#https://arxiv.org/pdf/1512.03385\n",
        "#Decided to copy what paper is doing with kernal sizes etc\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(Model, self).__init__()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.max_pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=5, stride=1, padding=2) # Input channels = 3 (RGB), output channels = 32, kernel size = 5x5, stride = 1, padding = 2 (to keep the spatial dimensions the same)\n",
        "\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)\n",
        "        self.bn4 = nn.BatchNorm2d(128)\n",
        "        self.conv5 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
        "        self.bn5 = nn.BatchNorm2d(128)\n",
        "        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, stride=1)\n",
        "        self.bn6 = nn.BatchNorm2d(128)\n",
        "        self.conv7 = nn.Conv2d(128, 256, kernel_size=3, stride=1)\n",
        "        self.bn7 = nn.BatchNorm2d(256)\n",
        "        self.conv8 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
        "        self.bn8 = nn.BatchNorm2d(256)\n",
        "        self.conv9 = nn.Conv2d(256, 256, kernel_size=3, stride=1)\n",
        "\n",
        "        #Channels are the amount of filters applied to our spatial data, giving depth to our feature map\n",
        "        #The spatial data will shrink because of 'edge issue' with CNN look at notes for more details\n",
        "        self.bn9 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.avg_global_pool = nn.AdaptiveAvgPool2d(1) # Global average pooling converts our (batch size, num_channels, height width) spatial dimensions into the average per channel across the height and width of the image.\n",
        "        #This makes our shape (batch_size, num_channels, 1,1) containing just the average per each channel across the image (height and width)\n",
        "        #for us its (batch_size, 64, 1,1)\n",
        "        #once flattened this becomes (batch_size, 64 (channels))\n",
        "        #This avoids us needing a fully connected layer with loads of parameters to deal with the large spatial dimensions\n",
        "\n",
        "        self.fc1 = nn.Linear(256, num_classes) #Classification layer\n",
        "    def forward(self, x):\n",
        "        # Input x is of shape (batch_size, 3, 512, 512)\n",
        "        x = self.conv1(x)\n",
        "        #Spatial data: output shape will be (batch_size, 64, 512, 512) because of padding=2\n",
        "\n",
        "        x = self.bn1(x) #Batch normalization to stabilize and speed up training by normalizing the activations of the previous layer\n",
        "        x = F.relu(x) #ReLU activation function to bring non-linearity to the model\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        #Spatial data: output shape will be (batch_size, 64, 510, 510) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x) removing dropout to increase stability of training, increasing L2 regularization and augmentation instead\n",
        "\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        #Spatial data: output shape will be (batch_size, 64, 508, 508) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.max_pool(x)  # Max pooling layer, reduces the spatial dimensions by half, you should only pool after a few convolutional layers\n",
        "        #This will take only the most important features from the feature map, and reduce the spatial dimensions to (batch_size, 64, 254, 254)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        #Spatial data: output shape will be (batch_size, 128, 252, 252) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "        # x = self.dropout(x) removing dropout to increase stability of training, increasing L2 regularization and augmentation instead\n",
        "\n",
        "        x = self.conv5(x)\n",
        "        #Spatial data: output shape will be (batch_size, 128, 250, 250) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn5(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv6(x)\n",
        "        #Spatial data: output shape will be (batch_size, 128, 248, 248) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn6(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.max_pool(x)  # Max pooling layer, reduces the spatial dimensions by half\n",
        "        #This will take only the most important features from the feature map, and reduce the spatial dimensions to (batch_size, 128, 124, 124)\n",
        "\n",
        "        x = self.conv7(x)\n",
        "        #Spatial data: output shape will be (batch_size, 256, 122, 122) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn7(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv8(x)\n",
        "        #Spatial data: output shape will be (batch_size, 256, 120, 120) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn8(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self.conv9(x)\n",
        "        #Spatial data: output shape will be (batch_size, 256, 118, 118) because of no padding and kernel size of 3\n",
        "\n",
        "        x = self.bn9(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        #At the end here our spatial data is 118*118*256 channels because each filter gives an output per location in the spatial data\n",
        "\n",
        "        #So we have 256 filters, each giving an output for each of the 118*118 locations in the spatial data\n",
        "\n",
        "        #This is a lot of data, and would require a huge fully connected layer to process it all, so we use global average pooling to reduce the spatial dimensions to an average per channel across the height and width of the image\n",
        "        #Making its output: (batch_size, 256, 1, 1)\n",
        "        x = self.avg_global_pool(x)\n",
        "\n",
        "\n",
        "        x = torch.flatten(x, 1)  # Flatten the output for the fully connected layer, keeping the batch dimension into (batch_size, 256)\n",
        "\n",
        "        x = self.fc1(x)  # Fully connected layer for classification\n",
        "\n",
        "        return x\n",
        "\n",
        "    def train_epoch(self, train_loader, loss_values):\n",
        "        running_loss = 0.0 #Running loss for the epoch, we will average it later\n",
        "\n",
        "        # Every data instance is an input + label pair, in a batch of 32\n",
        "        for i, data in enumerate(train_loader): #enumerate give us a counter as well as the data\n",
        "            inputs, labels = data #Get the inputs and labels from the batch\n",
        "            #Has to be .float for CrossEntropyLoss\n",
        "            inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # changes label to class indices (1,2,3,4) which is what CrossEntropyLoss wants, instead of one-hot\n",
        "\n",
        "            optimizer.zero_grad() # Zero your gradients for every batch!\n",
        "\n",
        "            outputs = self(inputs) # Make predictions for this batch (forward pass)\n",
        "\n",
        "            loss = criterion(outputs, labels) # Calculate the loss for this batch using the cost function defined earlier\n",
        "            loss.backward()  # Backpropagate the loss for this batch (backward pass)\n",
        "\n",
        "            optimizer.step() # Adjust learning weights with gradient descent, using the gradients from backpropagation (step the optimizer)\n",
        "            running_loss += loss.item() # running_loss is the total loss for the epoch, we will average it later\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader) # Average loss for the epoch (over the all batches in the training set, given by len(train_loader))\n",
        "\n",
        "        loss_values.append(avg_loss) #log the average loss for the epoch to the loss_values list\n",
        "        return avg_loss, loss_values"
      ],
      "id": "1afce429f0732539",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A4m01gQqrc-j"
      },
      "id": "A4m01gQqrc-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(4).to(device)  # 4 classes"
      ],
      "metadata": {
        "id": "KKwRICK45FVF"
      },
      "id": "KKwRICK45FVF",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "f1a923d7dcd71832"
      },
      "cell_type": "markdown",
      "source": [
        "L2 regularization to prevent overfitting, Adam optimizer for training the model, with a learning rate of 0.001 and weight decay of 1e-8 for L2 regularization\n",
        "This optimizer will update the model weights during training to minimize the loss function.\n",
        "\n",
        "Adams optimizer it gives each weight its own learning rate"
      ],
      "id": "f1a923d7dcd71832"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.227958Z",
          "start_time": "2025-08-26T13:54:51.225083Z"
        },
        "id": "adf1ac72e82f6519"
      },
      "cell_type": "code",
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4) #Increasing weight decay to 1e-4 to see if it helps with overfitting"
      ],
      "id": "adf1ac72e82f6519",
      "execution_count": null
    },
    {
      "metadata": {
        "id": "3f23d5ac26a4d21e"
      },
      "cell_type": "markdown",
      "source": [
        "This doesn't seem to be working properly, so disabling it for now, but keeping the code here in case we want to try it again later"
      ],
      "id": "3f23d5ac26a4d21e"
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:51.237154Z",
          "start_time": "2025-08-26T13:54:51.235043Z"
        },
        "id": "490b6e8c55ac0c58"
      },
      "cell_type": "code",
      "source": [
        "recent_train_losses = []  # List to keep track of recent training losses for early stopping\n",
        "previous_train_losses = []\n",
        "\n",
        "#This was broken - made a change idk now\n",
        "def early_stopping(avg_loss,patience=5):\n",
        "\n",
        "    if len(recent_train_losses) < patience:\n",
        "        recent_train_losses.append(avg_loss)  # Add the current average loss to the recent losses\n",
        "\n",
        "    if len(previous_train_losses) < patience:\n",
        "        previous_train_losses.append(recent_train_losses[0])\n",
        "        recent_train_losses.pop(0)\n",
        "\n",
        "    if len(recent_train_losses) == patience and len(previous_train_losses) == patience:\n",
        "        avg_of_previous_losses = np.mean(previous_train_losses)\n",
        "        avg_of_recent_previous_losses = np.mean(recent_train_losses)\n",
        "        if avg_of_previous_losses < avg_of_recent_previous_losses:\n",
        "            print(f\"Early stopping condition met: {avg_of_previous_losses} < {avg_of_recent_previous_losses}\")\n",
        "            return True, recent_train_losses, previous_train_losses  # Early stopping condition met\n",
        "\n",
        "        print(f\"Early stopping condition not met: {avg_of_previous_losses} >= {avg_of_recent_previous_losses}\")\n",
        "        previous_train_losses.append(recent_train_losses[0])  #Add oldest recent loss to previous losses\n",
        "        recent_train_losses.pop(0)  # Remove the oldest loss to maintain the size of the list\n",
        "\n",
        "    return False, recent_train_losses, previous_train_losses  # No early stopping condition met\n"
      ],
      "id": "490b6e8c55ac0c58",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-26T13:54:57.642679Z",
          "start_time": "2025-08-26T13:54:51.266832Z"
        },
        "id": "3132729a3bdcb6b9"
      },
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "#There's a bunch of tensorboard stuff in the pytorch tutorial, we don't need it for now, but we can add it later if we want to visualize the training process\n",
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "writer = SummaryWriter('runs/classifier_trainer{}'.format(timestamp))\n",
        "\n",
        "EPOCHS = 250\n",
        "\n",
        "best_vloss = 1_000_000. #Super big number to start with, so we can save the model if the validation loss is lower than this\n",
        "\n",
        "# Initialize lists to keep track of loss values for plotting later\n",
        "loss_values = []\n",
        "val_loss_values = []\n",
        "\n",
        "\n",
        "total_val_data = len(val_loader)\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch + 1)) # Start counting epochs from 1 for better readability\n",
        "\n",
        "    # Make sure gradient tracking is on, and do a pass over the data, so we are not in evaluation mode.\n",
        "    model.train(True)\n",
        "\n",
        "    #Actually train the model, this is where the training happens\n",
        "    #Keep track of loss values for plotting later, keep giving the loss_values array to the train_one_epoch function so that it keeps track of the loss values\n",
        "    avg_loss, loss_values = model.train_epoch(train_loader, loss_values)\n",
        "\n",
        "    early_stop,recent_train_losses, previous_train_losses = early_stopping(avg_loss,patience=5)  # Check if early stopping condition is met\n",
        "\n",
        "    running_vloss = 0.0\n",
        "    # Set the model to evaluation mode, disabling dropout and using population\n",
        "    # statistics for batch normalization.\n",
        "    model.eval()\n",
        "\n",
        "    #This is the validation loop, it will run after each epoch and compute the validation loss\n",
        "    # Disable gradient computation and reduce memory consumption.\n",
        "    with torch.no_grad():\n",
        "        for i, vdata in enumerate(val_loader, 0): #Computes the validation loss per epoch\n",
        "            vinputs, vlabels = vdata\n",
        "            vinputs, vlabels = vinputs.to(device), torch.argmax(vlabels, dim=1).to(device) #changes label to class indices (1,2,3,4) which is what CrossEntropyLoss wants, instead of one-hot\n",
        "            voutputs = model(vinputs)\n",
        "            vloss = criterion(voutputs, vlabels)\n",
        "            #running vloss is the total validation loss for the epoch, we will average it later\n",
        "            running_vloss += vloss.item()  # Add the validation loss for this batch to the running validation loss\n",
        "\n",
        "    # Average validation loss for the epoch (over the all batches in the validation set, given by len(val_loader))\n",
        "    avg_vloss = running_vloss / total_val_data\n",
        "    val_loss_values.append(avg_vloss)\n",
        "\n",
        "    #Gives the loss and validation loss for the epoch into the terminal\n",
        "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "\n",
        "    # for both training and validation w]e log the average loss for the epoch\n",
        "    writer.add_scalars('Training vs. Validation Loss',\n",
        "                       { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
        "                       epoch + 1)\n",
        "    writer.flush()\n",
        "\n",
        "    #Track best performance, and save the model's state\n",
        "    if avg_vloss < best_vloss: #If the validation loss is lower than the best validation loss so far, save the model\n",
        "\n",
        "        #Best vloss starts at a super high number, so the first model will always be saved\n",
        "        best_vloss = avg_vloss\n",
        "        model_path = 'model_{}_{}'.format(timestamp, epoch + 1) # Save the model with a timestamp and epoch number (epoch + 1 to start from 1)\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "#Disabling early stopping\n",
        "    # if early_stop:  # Check if early stopping condition is met\n",
        "    #     print(\"Early stopping triggered, stopping training.\")\n",
        "    #     break  # Stop training if early stopping condition is met\n",
        "\n",
        "    #Onto the next epoch, which will also run through all the batches in the training set and validation set and give the average los s for the epoch"
      ],
      "id": "3132729a3bdcb6b9",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "e40225ad38b16e4a"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Plot the training and validation loss\n",
        "plt.plot(loss_values, label='Training Loss')\n",
        "plt.plot(val_loss_values, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()"
      ],
      "id": "e40225ad38b16e4a"
    },
    {
      "metadata": {
        "id": "7185717e8222c208"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Training set evaluation\n",
        "from torcheval.metrics import MulticlassAccuracy #This lib has so many of these - good to use i think\n",
        "model.eval() #Turns off dropout layers\n",
        "metric = MulticlassAccuracy()\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # Move inputs and labels to the same device as the model\n",
        "        outputs = model(inputs)\n",
        "        preds = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
        "        metric.update(preds, labels)  # Update the metric with predictions and true labels\n",
        "\n",
        "accuracy = metric.compute()  # Compute the accuracy\n",
        "print(f'Training set accuracy: {accuracy:.4f}')  # Print the accuracy"
      ],
      "id": "7185717e8222c208"
    },
    {
      "metadata": {
        "id": "4fb4b141dbb4dbf2"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "#Test set evaluation\n",
        "model.eval()\n",
        "metric = MulticlassAccuracy()\n",
        "predictions = []\n",
        "labels_list = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), torch.argmax(labels, dim=1).to(device)  # Move inputs and labels to the same device as the model\n",
        "        outputs = model(inputs)\n",
        "        prediction = torch.argmax(outputs, dim=1)  # Get the predicted class indices\n",
        "        predictions.append(prediction)\n",
        "        labels_list.append(labels) #append all labels that appeared during the test set evaluation\n",
        "        metric.update(prediction, labels)  # Update the metric with predictions and true labels\n",
        "\n",
        "accuracy = metric.compute()  # Compute the accuracy\n",
        "print(f'Test set accuracy: {accuracy:.4f}')  # Print the accuracy"
      ],
      "id": "4fb4b141dbb4dbf2"
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"final_model_mutlilabel.pth\")  # Save the final model state dictionary"
      ],
      "metadata": {
        "id": "Iu6CfRc7tPcq"
      },
      "id": "Iu6CfRc7tPcq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot a confusion matrix (prepping data for it)\n",
        "#https://stackoverflow.com/questions/46953967/multilabel-indicator-is-not-supported-for-confusion-matrix\n",
        "#https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "\n",
        "# Concatenate all predictions and labels into a single tensor (currently they are lists of tensors)\n",
        "\n",
        "#Basically by counting the amount of each class number that appear in this tensor, we can get the confusion matrix  (cell below)\n",
        "all_predictions_tensor = torch.cat(predictions, dim=0) # Concatenate all predictions that appeared in the test set into a single tensor\n",
        "all_labels_tensor = torch.cat(labels_list, dim=0)  # Concatenate all labels that appeared in the test set into a single tensor\n",
        "\n",
        "\n",
        "# Convert to CPU tensors then to numpy arrays for confusion matrix\n",
        "all_predictions_tensor = all_predictions_tensor.cpu().numpy()\n",
        "all_labels_tensor = all_labels_tensor.cpu().numpy()\n",
        "print(all_predictions_tensor.shape, all_labels_tensor.shape)  # Check the shapes of the tensors\n",
        "print(all_predictions_tensor[0].shape, all_labels_tensor[0].shape) # Check the shape of the first prediction and label tensors\n",
        "\n",
        "\n",
        "#Data datatype before argmax, they are one-hot encoded tensors (boolean tensors) no good for confusion matrix\n",
        "print(all_predictions_tensor.dtype, all_predictions_tensor.dtype)\n",
        "\n",
        "# Argmax to convert one-hot encoded predictions and labels to a list of predicted and actual labels as integers\n",
        "print(all_predictions_tensor.dtype, all_labels_tensor.dtype)\n",
        "\n",
        "\n",
        "print(np.unique(all_predictions_tensor))\n",
        "print(np.unique(all_labels_tensor)) #Aah, there were only 7 unique labels in the test set\n",
        "# so we need to make sure the confusion matrix doesn't drop the 'empty labels' because the model might still have predicted them, but it could have been wrong\n"
      ],
      "metadata": {
        "id": "VftVv2kWtyF_"
      },
      "id": "VftVv2kWtyF_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_labels = df.columns[1:]  # Exclude the first column (img_name) for display labels\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels_tensor, all_predictions_tensor, labels=np.arange(len(display_labels)))\n",
        "#np.arrange basically just creates an array of length of display_labels (array of 15 elements, 0 to 14) which are the labels for the possible labels in the dataset, so the confusion matrix will have 15 rows and 15 columns, one for each label\n",
        "\n",
        "#Without display_labels the confusion matrix would only render the labels that were actually predicted, so if a label was never predicted, it would not appear in the confusion matrix, which is not what we want\n",
        "\n",
        "#15 labels, so 15x15 matrix\n",
        "print(cm.shape)\n",
        "\n",
        "matrix = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=display_labels) #disiplay_labels are the text labels (from column names) for the confusion matrix, these are the labels we want to display on the x and y axis of the confusion matrix, these are the same order as the labels we used for confusion matrix computation above,thanks to np.arrange(), so they will line up correctly\n",
        "\n",
        "_, ax = plt.subplots(figsize=(10, 10))  # Create a figure and axis for the confusion matrix plot (much larger than default\n",
        "matrix.plot(cmap='Blues', values_format='d', ax=ax, xticks_rotation=90)  # Plot the confusion matrix with blue color map and integer format\n"
      ],
      "metadata": {
        "id": "P5s6QuDou3WL"
      },
      "id": "P5s6QuDou3WL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_all_zeros = (df == 0).all(axis=1).sum()\n",
        "\n",
        "print(count_all_zeros)"
      ],
      "metadata": {
        "id": "aQF0gJ1jvoyX"
      },
      "id": "aQF0gJ1jvoyX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count rows where more than one column has value 1\n",
        "count_more_than_one_1 = (df == 1).sum(axis=1).gt(1).sum()\n",
        "\n",
        "print(count_more_than_one_1)"
      ],
      "metadata": {
        "id": "nLUFACw9wBGT"
      },
      "id": "nLUFACw9wBGT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}